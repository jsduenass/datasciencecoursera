[["index.html", "Data Science course notes Introduction", " Data Science course notes Juan Sebastian D 2021-04-13 Introduction A notebook through my learning journey These are notes taken and work done while coursing the Data Science specialization courses, A Coursera and John Hopkins joint effort towards teaching data science. "],["module1.html", "Chapter 1 Getting Started 1.1 final assessment", " Chapter 1 Getting Started In this module (The Data Scientists Toolbox), one is introduced to the basics of what data science is, some of the topics discussed are: types of data, the job of the data scientist, and the process needed in order to cure data and information into usable knowledge. It also has a practical component, introducing you to an important tool at the data scientist disposal, the R language and its IDE Rstudio. The subject of version control is also addressed and the basics of git and github are explore. The lectures were made following the spirit of reproducible work and R potential use for automation the material consisted of written lectures and videos made completely autonomously extracting the information of the written material and putting it in video format. This stroke my curiosity, the videos made deliver far from perfect experience, but it displayed a practical application of what can be done with R. One of the pieces of content that strike me as meaningful was at the introduction, answering the question why we need data science: One of the reasons for the rise of data science in recent years is the vast amount of data currently available and being generated. Not only are massive amounts of data being collected about many aspects of the world and our lives,but we simultaneously have the rise of inexpensive computing. This has created the perfect storm in which we have rich data and the tools to analyze it, rising computer memory capabilities, better processors, more software and now, more data scientists with the skills to put this to use and answer questions using this data. 1.1 final assessment As part of the final assessment one needs to demonstrate that has successfully set up a R installation with a working Rstudio. for me both installations went flawless and Rstudio detected the R program without any configuration issues. Rstudio screenshot One also need to demonstrate the creation of a github account mine can be found as jsduenass. create a markdown file and make a fork from jtleeks How to share data with a statistician I found interesting the peer review methodology used in this final assignment that encourage students participate and interact. "],["module2.html", "Summary", " Summary About R Programming was the second course and deal more deeply in the fundamentals of using R, the basic syntaxes and data structures, like list and dataframes and basic logic structures like loops, if statements and functions. "],["getting-acquainted-with-r.html", "Chapter 2 Getting acquainted with R 2.1 Looking at Data 2.2 Sessions and environment", " Chapter 2 Getting acquainted with R These lectures concentrated on basic R syntax, different data types, some built-in functions and methods for reading data from external files. Thanks to the RStudio Team (2020) for providing a intuitive interface to work with R. R is an object orientated programming language focus around statistics programming and data analysis, the way it work is that it store each instance in memory so it can refer it later. R comes with some predefined variables like LETTERS and letters , which holds a list with the capitalized and non capitalized letters of the alphabet, constants such pi and i and j for imaginary numbers. Some interesting little behavior that happen (like in many other programming languages), is that do to limited precision on floating points sometimes results that doesnt look zero actually mean 0. letters cos(pi/2) 1+0i -3i head(letters) The ? symbol and help can be used to access the documentation page of built-in functions. Some other functions great for getting information about functions are: the args() and attributes() which gives the arguments and attributes of an object respectably. ? dimnames() help(&quot;mean&quot;) ls() # list of variables args(read.table) # get arguments from a function my_vector &lt;-c(1,2) attributes(my_vector) # get the attributes of an object class(my_vector) There are two methods to print the value of a variable both print and cat, print is a generic function that can be implemented across user-defined objects. While cat display a character string of the parameter pass to it. age&lt;- 71 print(age) ## [1] 71 cat(age) ## 71 When working with R, you have an assigned working directory which usually is initialize at the directory where your R project is located. There are functions that let you interact with the file system and individual files. getwd() # get working directory path.expand(&quot;~&quot;) # shows the location of the root directory # setwd(~) # set working directory to root directory list.files() file.info(&#39;index.Rmd&#39;) Some easy key shortcut in order to keep a clean console and workspace always come in handy. control + L clears the console and remove(list=ls()) erases all the variables stored in the environment __ This is a potentially dangerous__ because it can erase all your specially if you have been using scripting in the console instead of code written on files but can be useful to give new breathing air when you work is getting to clutter. 2.1 Looking at Data The following functions allows you to get a look at different aspects of an object, they become very useful when dealing with datasets and doing exploratory analysis. object.size() allows you to know how much space a certain object is occupying in memory mostly used with datasets. str() display the structure of an object summary() is very useful to get a sense of the data it give properties such as the mean, the median, the number of NA values, etc. table() display information in a table format dim() show the dimension of a dataframe names() show the names of the columns of a datafram 2.2 Sessions and environment Sessions store information about the working directory, this configuration can be seen in the .Rdata file. environments are the container where objects are stored. Objects created by the user get stored in the global environment, core functions are store in the base environment and every library loaded has its own environment. When R wants to find the value/definition of an object it searches trough this environment in a particular order starting by the global one. When evaluating a function R creates its own environment, that is why variables created inside a function have no effect in the main environment. environments are a collection of symbol/value pairs globalenv() baseenv() emptyenv() parent.env(globalenv()) References "],["data-types.html", "Chapter 3 Data types 3.1 Exotic data types 3.2 Data conversion 3.3 matrices 3.4 factor 3.5 Data frames", " Chapter 3 Data types R support different atomic object classes, this can be seen as data types. these are: character numeric integer complex logical Note: When writing numbers they are numeric as default, but can forced to become integer type by ending the number with an L. It is interesting to note that the integer type makes part of the numeric one this can be seen using the is.numeric function member of the is.x() family of functions which allow to identify if a variable is of a particular class. class(10) ## [1] &quot;numeric&quot; class(10L) ## [1] &quot;integer&quot; is.numeric(10L) ## [1] TRUE 3.1 Exotic data types there as well some not as common data types such as date, raster, raw and others. raw for example creates a vector used for storing information in binary form into bytes written as hexadecimal pairs. text &lt;- &quot;a word with length 21&quot; binary &lt;- charToRaw(text) is.raw(binary) ## [1] TRUE typeof(binary) ## [1] &quot;raw&quot; 3.2 Data conversion Data types have some hierarchy, from particular to general they can be organized as follows logical, integer, numeric, character. One can perform data type conversion from a particular type to a more general one. R support implicit conversion in order to keep types consistence. For example all of the elements of a matrix must be of the same type, when different types are used inside a matrix R automatically does this conversion so they all are the same type. Manual conversion can also be done using the as.x() family of functions. as.character(5) ## [1] &quot;5&quot; as.numeric(TRUE) ## [1] 1 as.logical(1) ## [1] TRUE On top of the atomic object, R has the following basic data structures: vector, list, matrix, data frame, and factors. 3.2.1 vector Is an array of elements all of them of the same type, there are different ways to create a vector, an empty one through the vector function, by the concatenate method c() or by a sequence : x&lt;-vector(mode=&quot;character&quot;,length=11) # empty vector y&lt;- c(1,5,7,3) # concatenated elements z&lt;- 10:20 # sequence When mixing elements of different classes into a vector there are coercion rules that come into place to make sure the all the same rule applies. Peng (2019) x&lt;-c(1, TRUE, &quot;hello&quot;) x ## [1] &quot;1&quot; &quot;TRUE&quot; &quot;hello&quot; Note that all the values of the vector have transform into character type. 3.2.2 Vector operations When programing you want to perform operations over your variables,when dealing with vector operations are made element by element. One thing to be cautious about is that when doing operation with vectors of different size R repeats the smaller vector until their sizes match. they need to be multiplies of one another in order to work, otherwise it would produce an error. This behavior extends the Rs expressiveness and language functionality, but can also can create difficult to spot unintended behavior if not treated carefully. x &lt;- 1:4 x*x x^x x^(1:2) M&lt;-x%o%x det(M) t(M) seq(1,10, by=0.3) seq(1,10, length=7) seq(along.with=x) seq_along(x) 4%in%x 3.2.3 vector indexing When working with vectors there can be numerical indexing (where elements of the array are retrive if it matches its position) as well as logical indexing where the element is retrieve if the condition set, is satisfy by the element. x&lt;-10:20 x[2] ## [1] 11 x[2:6] # position indexing ## [1] 11 12 13 14 15 x[x%%3==0] # logical indexing, ## [1] 12 15 18 # return all x whose module by 3 is equal to 0 ##list Are an special type of vector which can contain elements of different type. L&lt;-list(1, TRUE, &quot;hello&quot;) L ## [[1]] ## [1] 1 ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] &quot;hello&quot; L&lt;- vector(mode=&quot;list&quot;, length=3) L ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL a list can name each of its elements. One example I imagine was to create a list which represents the dimensions of a object, so each value represents the height, width and depth of the object. dimensions&lt;- list(height=5, width=4, depth=3) names(dimensions) ## [1] &quot;height&quot; &quot;width&quot; &quot;depth&quot; 3.3 matrices Are constructed column-wise, but can be set to be row-wise by the parameter byrow = TRUE. x &lt;- 1:10L y &lt;- 11:20 M &lt;-cbind(x,y) typeof(M) dim(M) Mt&lt;- rbind(x,y) A &lt;- matrix(1:10, nrow = 2, byrow = TRUE) An descriptive name can be useful to understand better the data presented, that is why R allows Matrix to have can have both rownames and colnames M &lt;-matrix(1:4, ncol=2) dimnames(M)&lt;- list(c(&quot;a&quot;,&quot;b&quot;),c(&quot;c&quot;,&quot;d&quot;)) M ## c d ## a 1 3 ## b 2 4 colnames(M)&lt;-c(&quot;col1&quot;,&quot;col2&quot;) rownames(M)&lt;- c(&quot;row1&quot;,&quot;row2&quot;) M ## col1 col2 ## row1 1 3 ## row2 2 4 3.4 factor Are variables that allow to easily describe categories. For example imagine you make some poll to measure peoples satisfaction with some policy. Normally this types of question doesnt just have a numeric value to be rated but can be more descriptive like for example having the options [really bad,bad,good,really good] when asking about how they feel the policy has been carried out. factor data types allow to easily express these subtle descriptive answers. if some analysis needs to be carried out and assigning a value to each one for example to find the mean of the answers. they can easily be ranked from lower to higher using levels. x&lt;- factor(c(&quot;yes&quot;,&quot;no&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;no&quot;)) table(x) ## x ## no yes ## 4 4 unclass(x) ## [1] 2 1 2 1 2 2 1 1 ## attr(,&quot;levels&quot;) ## [1] &quot;no&quot; &quot;yes&quot; answers &lt;- c(&quot;good&quot;,&quot;bad&quot;,&quot;really good&quot;,&quot;really bad&quot;,&quot;bad&quot;,&quot;really bad&quot;,&quot;good&quot;) satisfaction &lt;- factor(answers, levels=c(&quot;really bad&quot;,&quot;bad&quot;,&quot;good&quot;,&quot;really good&quot;)) print(satisfaction) ## [1] good bad really good really bad bad really bad ## [7] good ## Levels: really bad bad good really good 3.5 Data frames Data frames are structures design to store tabular data, most of the time they are used to store datsets read from files but they can also be explicitly created by the data.frame command. For example imagine you want to run a poll and store the data of each participant for further analysis, then a data frame structure can be used to organize this information. opinion_poll &lt;- data.frame(name=c(&quot;karen&quot;, &quot;brayan&quot;,&quot;Tony&quot;), age=c(27,36,38),opinion=c(&quot;agree&quot;,&quot;disagree&quot;,&quot;agree&quot;)) print(opinion_poll) ## name age opinion ## 1 karen 27 agree ## 2 brayan 36 disagree ## 3 Tony 38 agree names(opinion_poll) ## [1] &quot;name&quot; &quot;age&quot; &quot;opinion&quot; names(opinion_poll) &lt;- NULL newnames&lt;- c(&quot;first name&quot;,&quot;age&quot;,&quot;opinion&quot;) colnames(opinion_poll)&lt;- newnames you can always erase the names by setting them to null. Rows names can be set as well by row.names 3.5.1 Dollar sign and double brackets The single bracket notation will give back a subset of the same element, if it is a data frame it will return a data frame and if it is a list it returns a list. however if the data is needed without the structure as it might be needed to properly work with some functions a double bracket remove the structure leaving only the data. the dollar sign notation have a similar behavior than the double bracket, but allows the values to be accessed by their name Grolemund (2014) opinion_poll[2] ## age ## 1 27 ## 2 36 ## 3 38 opinion_poll[[2]] ## [1] 27 36 38 # mean(opinion_poll[2]) will produce an error mean(opinion_poll[[2]]) #will not ## [1] 33.66667 mean(opinion_poll$age) ## [1] 33.66667 median(opinion_poll$age) ## [1] 36 References "],["miscellaneous.html", "Chapter 4 Miscellaneous 4.1 Missing data NaN, NA and NULL 4.2 functions 4.3 plot 4.4 R Packages 4.5 Read info with read table 4.6 week assignment", " Chapter 4 Miscellaneous 4.1 Missing data NaN, NA and NULL The world tends to be messy and missing data tends to be the norm instead of the exception, Rs way of dealing with this is through defining the NaN and NA data types. NaN is more specific and used when the missing values is a numeric one, NA on the other hand is more general and cover any missing value. NA has a more broader definition, therefore NaN values are considered to be covered by NA, therefore any NaN is simultaneously NA as well. For example, the following the functions is.na() and is.NaN() are used to identify this types of data, the NaN value is recognized to be TRUE for both functions. answers &lt;- c(&quot;good&quot;,&quot;bad&quot;,&quot;really good&quot;,&quot;really bad&quot;,&quot;bad&quot;,&quot;really bad&quot;,&quot;good&quot;) satisfaction &lt;- factor(answers, levels=c(&quot;really bad&quot;,&quot;bad&quot;,&quot;good&quot;,&quot;really good&quot;)) satisfaction[3]=NA satisfaction[4]=NaN # numeric not define is.na(satisfaction) ## [1] FALSE FALSE TRUE TRUE FALSE FALSE FALSE is.nan(satisfaction) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE Other important data type related with missing values is null, while it might prompt confusion, Id like to refer to the following saying to address the difference: NA represents the absence of presence while null represents the presence of absence NA values can create a lot of trouble, many operations are not defined over this type of data and when apply can render a whole dataset with unusable results. This is a known issue so many functions support a mechanism for dealing with missing values, the na.rm parameter removes the missing values before the function is evaluated. mean(c(1,2,NaN,4,NA,6)) # the operation is undefined due to the presence of NA ## [1] NaN mean(c(1,2,NaN,4,NA,6), na.rm = TRUE) ## [1] 3.25 Similar tricks and workarounds can be used in order to remove the missing values of a dataset, they exploit the use of logical indexing in order to remove any element that is NA. data&lt;- c(1:4, NA ,6:10, NaN, NA ,1 , NA, 3:6) missing &lt;-is.na(data) clean_data&lt;- data[!missing] data2&lt;- c(3:7, 6:10,NA ,3:1, NaN, NA,1,5 ) good&lt;- complete.cases(data,data2) data[good] ## [1] 1 2 3 4 6 7 8 9 10 1 5 6 data2[good] ## [1] 3 4 5 6 6 7 8 9 10 2 1 5 When working with data frames the complete.cases can be useful for handling missing values, it return a logical vector whose value are TRUE or FALSE depending if all of the elements of a row are defined and none is NA, na.omit is an alternative way to accomplish the same result, instead of returning a logic vector it directly returns the rows that does not have missing values. df&lt;-data.frame(a=1:4, b=2:5, c=10:13,d=21:24) df[1,3]&lt;-NA df[3,4]&lt;-NA df ## a b c d ## 1 1 2 NA 21 ## 2 2 3 11 22 ## 3 3 4 12 NA ## 4 4 5 13 24 good&lt;-complete.cases(df) good # logic vector data is complete ## [1] FALSE TRUE FALSE TRUE clean_df&lt;-df[good,] clean_df ## a b c d ## 2 2 3 11 22 ## 4 4 5 13 24 alternative&lt;-na.omit(df) alternative ## a b c d ## 2 2 3 11 22 ## 4 4 5 13 24 There are many functions which make part of thena.x() family, which provide a rich range of responding actions when encountering with missing values, like any na.fail()``` which triggers an alert if the element contains an NA value, as well asna.contiguous()``` which identifies the longest contiguous sequence of NA values. 4.2 functions When programming you always want to break big problems into smaller ones, and functions create the perfect container to hold each of these little pieces. R as mentioned previously is a programing language orientated towards statistical analysis therefore is has already bake in so useful functions, for example sample() can get a random sample with the specified size, by default there is no replacement but this behavior can easily be modified. sample(x = 1:4, size = 2) ## [1] 4 1 factorial(5) ## [1] 120 y &lt;-sample(x = 1:4, size = 100, replace=TRUE) PI&lt;-round(pi, digits = 2) # clear the clutter and remove objects remove(PI) y&lt;- rnorm(1000) #normal distribution 4.3 plot Other useful tool that R offers and is known for is the ability to draw diagrams. Plots are a fundamental way of conveying meaningful information from a pile of data. And plots play a key role in Rs philosophy of reproducible research. library(&#39;ggplot2&#39;) x &lt;- c(-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1) y &lt;- x^3 qplot(x, y) Figure 4.1: cubic function Another great phrase bias can easily occur without anyone noticing it in the short run. - Grolemund (2014) 4.3.1 practical exercise Wickham and Grolemund (2017) shows some practical plotting examples using the tiddyverse, a collection of very useful packages for data manipulation. If you want to know more about packages you can head to the corresponding section. We are going to use some practical data provided by the EPA (Enviromental Proteciton Agency), the mpg dataframe which makes part of the tidyverse and is used as go to introductory dataset. library(tidyverse) ?mpg names(mpg) dims(mpg) 4.4 R Packages One of the advantages of open source is that the community can creates packages which extends the functionality of the core R implementation, allowing for special niche use cases to be develop and satisfy by people in that circle without being cram into the base program every body uses, it allows to have the flexibility to customize and pick the tools needed while tapping into the work done by a large community of people. There exist many ways to access this packages, the larger collection is provided via the CRANN repository but Git based packages are also supported. In order to use any external package it is necessary not only to install it, but also to load it into the R environment. CRAN based package: CRAN (Comprehensive R Archive Network) is an collection that holds documentation and code related to R. It stores many R packages which have the advantage that they can be install directly via the install.packages(\" \") command making it extremely easy to incorporate new packages into a project. For example in order to install the tidyverse package collection the following command can be use: install.packages(&quot;tidyverse&quot;) library(tidyverse) Once installed the library() command can be used to load the packages into R. Notice that install.packages() gets an argument between quotes but library() does not. Note: It is important to note that in order to install some packages, the ones who have c/c++/fortran source code instead of the binary, they need the rtools in order be build, and the rtools must be compatible with the R version. Git based package: Note: packages needs to be loaded every time Rstudio is loaded. The following commands show you the loaded packages, and in the case you need to unload a package the detach command can be helpful. (.packages()) detach(&quot;package:tidyverse&quot;, unload=TRUE) Usually when dealing with a new package you want to know what is inside what dataset, functions or functionalities it provide, the help command ? can be use to explore the package documentation and get a better idea of what it contains. The following example allows to explore the datasets package, an built-in package that store some useful datasets. library(datasets) ?datasets library(help = &quot;datasets&quot;) # open file with full index of datasets The following is a list with some of these datasets: AirPassengers Monthly Airline Passenger Numbers 1949-1960 BJsales Sales Data with Leading Indicator BOD Biochemical Oxygen Demand CO2 Carbon Dioxide Uptake in Grass Plants ChickWeight Weight versus age of chicks on different diets DNase Elisa assay of DNase EuStockMarkets Daily Closing Prices of Major European Stock Indices, 1991-1998 We can take a look into some of this datasets head(EuStockMarkets) ## DAX SMI CAC FTSE ## [1,] 1628.75 1678.1 1772.8 2443.6 ## [2,] 1613.63 1688.5 1750.5 2460.2 ## [3,] 1606.51 1678.6 1718.0 2448.2 ## [4,] 1621.04 1684.1 1708.1 2470.4 ## [5,] 1618.16 1686.6 1723.1 2484.7 ## [6,] 1610.61 1671.6 1714.3 2466.8 head(AirPassengers) ## [1] 112 118 132 129 121 135 tail(ChickWeight) ## weight Time Chick Diet ## 573 155 12 50 4 ## 574 175 14 50 4 ## 575 205 16 50 4 ## 576 234 18 50 4 ## 577 264 20 50 4 ## 578 264 21 50 4 4.4.1 create your own package There are many things to consider when thinking about building a package. Why packages are important? Code organization Consistent documentation Code distribution why am I creating a package? It is about code distribution, For me I am not planning on creating some useful code to mankind -not yet-, I want to create an example package that will in the basics of package creation and would allow my to access some custom scripts across different computers, and want to configure all the needed dependencies. library(devtools) ## Loading required package: usethis library(roxygen2) # locate yourself in the parent directory create(&quot;simpleRpackage&quot;) here::dr_here() load_all() say_hi() or to make it permanent use the function install install(&quot;cats&quot;) library(cats) say_hi() minimum viable product  its best to get a project started and improve it through iteration Never use library() or require() in a R package! 4.5 Read info with read table Data can be produce in different forms, it can be structured or unstructured, regardless of the format, being able to interact with this data and organize it in a useful manner is an important process that needs to be done, so it can be feed to upcoming analysis. this process of organizing the data is known as pipelining (to learn more about this process feel free to check the Data collection module ). Some common formats for data storage are csv (Coma Separated Values) files, R has some built in function for reading this type of files through the read.csv() or read.table(). Here is an examples of some students data read from an csv file. These function have some parameters that help to specify the behavior we want it to have. For example one can determine what symbol would be used as separator or either if the file has headers or not. for more info on other supported arguments please read the documentation. One thing to be aware when dealing with large dataset is RAM, if the dataset is larger than the RAM memory capacity it will create troubles. initial &lt;- read.csv(&quot;../data/student_info.csv&quot;,sep=&quot;;&quot;,header=TRUE, nrows=100) classes&lt;- sapply(initial,class) students &lt;- read.table(&quot;../data/student_info.csv&quot;,sep=&quot;;&quot;,header=TRUE, colClasses = classes) print(students) ## Name.and.Lastname ID email.adress ## 1 Samuel Jackson 10124585611 samuelJ@mail.com ## 2 Robert Downey 4525111559 rdownironman@mail.c ## 3 Douglas Adams 14884674721 zaphodbeeblebrox@mail.com ## 4 Stephen Wolfram 74682868914 wolframalpha@mail.com ## 5 Cleve Moler 17463589721 chiefmathematician@mail.com ## 6 Matt Parker 18457956247 parkersquare@mail.com ## 7 Brady Haran 17694858874 numberphile@mail.com ## 8 Emily Graslie 17973595287 brainscoop@mial.com ## 9 Derek Muller 17954965491 veritasium@mail.com ## 10 Destin Sandlin 17895782879 smarter@mail.com ## 11 Freddy Vega 17795795697 fredier@mail.com ## 12 Stanislaw Lem 19748659471 golem@mail.com ## 13 Isaac Asimov 13589844557 robots@mail.com ## 14 Susan Calvin 47958446526 susan@R_MM_Inc.gov ## 15 R Daneel Olivaw 10001110101 dolivaw@mail.com ## 16 Elijah Baley 97565841047 lijahbaley@mail.com Here is a second example where one displays the lasts elements of the file. external_deck &lt;- read.csv(&quot;../data/deck_of_cards.csv&quot;) tail(external_deck,5) ## face suit value ## 48 five hearts 5 ## 49 four hearts 4 ## 50 three hearts 3 ## 51 two hearts 2 ## 52 ace hearts 1 When working with large data sets specifying the data type by the parameter colClasses, can optimized and reduce the time needed to process the data. 4.6 week assignment As part of these weeks assignment it was optional to do the following swirl practices. swirl (Statistics with Interactive R Learning) is a R package develop to allow students to learn by interacting directly with the command interface. In order to use it one needs to install the swirl package see. install.packages(&quot;swirl&quot;) library(swirl) References "],["scripting.html", "Chapter 5 Scripting 5.1 functions 5.2 Scoping rules 5.3 What happened in 1970-01-01? 5.4 SWIRL practices", " Chapter 5 Scripting Now that we have get acquaintance with the R language, some of its built-in functions, data types and structures, its time to learn about the building blocks of programming: control structures, cycles, and functions. Like many other programming languages R implements the if statement as a control structure and the for statement and while statement in order to produce cycles. there also exists the expressions repeat, break and next which allow to specify a certain behavior different from the rest of the iteration, there are usually used inside an if statement to represent exception cases inside a loop. number=1 if(number&gt;0){ print(&quot;the number is positive&quot;) }else if (number &lt;0){ print(&quot;the number is negative&quot;) }else { print(&quot;the number is 0&quot;) } ## [1] &quot;the number is positive&quot; x&lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) for(i in seq_along(x)){ print(x[i]) } ## [1] &quot;a&quot; ## [1] &quot;b&quot; ## [1] &quot;c&quot; 5.1 functions functions are structures that allow for encapsulation and reutilization of an action. It allows programmers to use the same process under different condition using its inputs. The declaration of a function is as follows. countdown &lt;- function (n){ for(timeleft in n:1) { cat(&quot;T minus: &quot;, timeleft, &quot;\\n&quot;) Sys.sleep(1) } cat(&quot;BOOM!! \\n&quot;) } countdown(3) ## T minus: 3 ## T minus: 2 ## T minus: 1 ## BOOM!! countdown(5) ## T minus: 5 ## T minus: 4 ## T minus: 3 ## T minus: 2 ## T minus: 1 ## BOOM!! So as you see we define the countdown function by assigning the reserved-word function with the parameters to the functions name, followed by the declaration of the functionality, contained between curly brackets. Finally we call the function in order to make it work. As seen we can use cat to display values, and this prompts the question: what is the difference between cat and print ? cat in the one hand converts its arguments to character vectors, concatenates them to a single character vector community (n.d.) these impose some limitations. It can only be applied to primary data types. Also it doesnt produce a new line, so \\n has to bee specified in order to do so. The returned value of this function is an invisible NULL. print in the other hand is a generic function which means that new printing methods can be easily added for new classes community (n.d.). different from cat, print returns its value in an invisible form which makes it useful for pipping. 5.1.1 defining arguments R allows for great flexibility when dealing with arguments in a function, some of this behaviors are: Lazy evaluation: This means that values are only evaluated when needed, this allows R to declared non defined arguments with out a problem as long as they are not used. default values: One can set arguments to have default values, when they are not specified in the call of the function they are set to the default value. This can become useful when dealing with many parameters who are intended to have to same value must of the time. partially match: Arguments can be assigned position based fun(value1,value2,value3) or name based fun(arg2=value2,arg3=value3,arg1=value1), name based allows to put in the arguments disorderly, however names can be to long, so a partial argument name can be used if it uniquely identifies the argument. unspecified arguments: Sometimes you can not predict what other arguments might applied to the function, the three dot  argument can be used to assign unspecified arguments. This is usually used when extending an existing function. One thing to be aware of when using is that the remainder arguments after the  argument must be named or else they will be taken as part of  also partial matching doesnt work as well. simon_says &lt;- function(...){ paste(&quot;Simon says:&quot;, ...) } this function example was taken from swirls functions module. unpacking arguments: when working with the  one might need to extract an specific value. One can access these values by converting the  into a list f &lt;- function(x, ...) { args = list(...) print(args$message) } f(x,name=&quot;Pedrito&quot;,message=&quot;hola&quot;) ## [1] &quot;hola&quot; All of these behaviors enhance user experience by allowing high level functionalities from functions. 5.2 Scoping rules R holds each definition in memory as a symbol value pair. however the same symbol might be defined differently in in two separated occasion, so how does R determine which one to use? This issue can be resolve thanks to scoping rules, which sort out the value of a symbol by following a hierarchy of environments that can be display by the command search(). First the symbol is look in the global environment, in second place it is search in the loaded libraries and then the rest of the build packages. R separates functions and function so you can have one with each with the same name. external= &quot;I am from the outside world&quot; f&lt;-function(){ print(external) } f2&lt;- function(){ external=&quot; I am inside function 2&quot; f() } f() ## [1] &quot;I am from the outside world&quot; f2() ## [1] &quot;I am from the outside world&quot; external= &quot;I am from the outside world&quot; f&lt;-function(){ print(external) } f2&lt;- function(){ f&lt;-function(){ print(external) } external=&quot; I am inside function 2&quot; f() } f() ## [1] &quot;I am from the outside world&quot; f2() ## [1] &quot; I am inside function 2&quot; In the previous example we saw that the function could use a variable that was not declared inside the function these are known as free variables. The scoping rules state that free variables would be search inside the environment where the function was defined, if not found, it is search up to the parent environment the example intends to be a reflection of this behavior. what value does a free variable has is tight to what environment it is on. the ls() function can help you see what is inside an environment, and the function get() what value does it have. ls(environment(f)) ## [1] &quot;age&quot; &quot;alternative&quot; &quot;answers&quot; &quot;binary&quot; ## [5] &quot;classes&quot; &quot;clean_data&quot; &quot;clean_df&quot; &quot;countdown&quot; ## [9] &quot;data&quot; &quot;data2&quot; &quot;df&quot; &quot;dimensions&quot; ## [13] &quot;external&quot; &quot;external_deck&quot; &quot;f&quot; &quot;f2&quot; ## [17] &quot;good&quot; &quot;i&quot; &quot;initial&quot; &quot;L&quot; ## [21] &quot;M&quot; &quot;missing&quot; &quot;newnames&quot; &quot;number&quot; ## [25] &quot;opinion_poll&quot; &quot;satisfaction&quot; &quot;simon_says&quot; &quot;students&quot; ## [29] &quot;text&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; get(&quot;external&quot;,environment(f)) ## [1] &quot;I am from the outside world&quot; get(&quot;external&quot;,environment(f2)) ## [1] &quot;I am from the outside world&quot; Do to lexical scoping all variables must be stored in memory. 5.3 What happened in 1970-01-01? The first of January of 1970 is an important date and marks the start of time itself, at least as far as computers are concerned. This date was set as the start of the unix epoch a system that measures time based on the number of seconds past since this date. This system has been used widely and now the unix time (POSIX) is the primary way computers deal with time. R has two ways of dealing with time: Dates: which can be represented using the Date class count the number of days since the unix epoch. x&lt;-as.Date(&quot;1978-12-1&quot;) class(x) ## [1] &quot;Date&quot; unclass(x) ## [1] 3256 Time: which uses POSIX can become in two different standards. POSIXct which works deep down as a integer counter, it is to work with data frames; and POSIXlt which stores the time in a list structure, with much more baggage information that can be easier to access. now &lt;-Sys.time() now ## [1] &quot;2021-04-13 19:31:03 -05&quot; now_ls &lt;- as.POSIXlt(now) names(unclass(now_ls)) ## [1] &quot;sec&quot; &quot;min&quot; &quot;hour&quot; &quot;mday&quot; &quot;mon&quot; &quot;year&quot; &quot;wday&quot; &quot;yday&quot; ## [9] &quot;isdst&quot; &quot;zone&quot; &quot;gmtoff&quot; now_ls$year ## [1] 121 As the example showed the time is relative to 1-1-1970 We have shown so far how the system interprets dates and times but imagine you want to enter dates that are written as strings. strptime() take care of this, it only needs the string and the format. It also allows to specify the time zone if needed. # ?strptime str_date= &quot;4 de julio de 1991&quot; date=strptime(str_date, format=&quot;%d de %B de %Y &quot;,tz=&quot;GMT&quot;) class(date) ## [1] &quot;POSIXlt&quot; &quot;POSIXt&quot; date+365 ## [1] &quot;1991-07-04 00:06:05 GMT&quot; date$mon ## [1] 6 5.4 SWIRL practices As part as this weeks assignment one needed to complete the swirl practices of logic, functions and Dates and Times. 5.4.1 logical operations: when using single operand the value gets evaluated by each one of the elements of the list, double operand evaluates only the first element. TRUE &amp; c(TRUE, FALSE, FALSE) ## [1] TRUE FALSE FALSE TRUE &amp;&amp; c(TRUE, FALSE, FALSE) ## [1] TRUE 5.4.2 functions: Anonymous functions. evaluate(function(x){x+1}, 6) binary operator convolution 5.4.3 Dates and times If needed the lubridate package developed by Hadley Wickham enhances the possibilities of the basic Date class. dates might seem easy at first glance but as it was put by Wickham and Grolemund (2017) Dates and times are hard because they have to reconcile two physical phenomena (the rotation of the Earth and its orbit around the sun) with a whole raft of geopolitical phenomena including months, time zones, and DST. So it is important to appreciate the work others have put in, in order to make it easier to work with the messy way we measure time, so you can focus on whatever you are working on rather that getting distracted trying to format dates information. References "],["loop-functions.html", "Chapter 6 Loop functions 6.1 Debbuging and error handling 6.2 R objects 6.3 Create package documentation 6.4 SWIRL practices", " Chapter 6 Loop functions lapply sapply tapply mapply apply: lets set margins gl 6.1 Debbuging and error handling message: informative warnings: unexpected behavior, display once execution has finished error: fatal breaks the program condition: generic out printing to the console useful tools traceback debug browser trace recover blunt techniques insert cat/ print staments insid the function mean(x) traceback() debug(lm) lm(x-y) undebug(lm) optins(error=recover) optins(error=browser) optins(error=NULL) tidyverse_update() &lt;&lt;- is an operator used to assign a object to a different environment 6.2 R objects 6.3 Create package documentation Run devtools::document() (or press Ctrl/Cmd + Shift + D in RStudio) to convert roxygen comments to .Rd files. 6.4 SWIRL practices 6.4.1 lapply sapply loop functions or *apply functions split apply combine strategy lay out by Hadley Wickham in his paper titled The Split-Apply-Combine Strategy for Data Analysis. Rows observation and columns variables specify its own anonymous function lapply(unique_vals, function(elem) elem[2]) back in week to we read some data from a file. Imagine we want to analyze this information we want to know what type is. first we try simply to find the class of students which aint very us Now we want to iterate operation over it lapply returns a list type as a result. sapply tries to simplify whenever is possible. iF every result is of length 1 it returns a vector if they are all bigger than one and with the same length it returns a matrix. students &lt;- read.table(&quot;../data/student_info.csv&quot;,sep=&quot;;&quot;,header=TRUE,nrows=5) class(students) ## [1] &quot;data.frame&quot; cls&lt;- lapply(students,class) 6.4.2 vapply and tapply vapply allows to specify the format of the result and throughs an error if not matched. more in depth of the source function source from web github print(&quot; I am filed called from a URL&quot;) ## [1] &quot; I am filed called from a URL&quot; source(&quot;https://raw.githubusercontent.com/jsduenass/datasciencecoursera/master/code/hello_I_am_from_the_web.R&quot;) ## [1] &quot;In R, the source function allows to call either files path names or URL connettion&quot; One can call an R script and all its content using source. R can also interface with different files compress files using  and web pages using the url function. #remove(list=ls()) #source(&quot;code/simple_script.R&quot;, encoding = &#39;UTF-8&#39;) #eval(parse(&quot;simple_script.R&quot;, encoding=&quot;UTF-8&quot;)) #roll() #dput(rank) #dump(c(rank,rolls)) content &lt;- file(&quot;../data/student_info.csv&quot;,&quot;r&quot;) some_lines &lt;-readLines(content,5) print(some_lines) ## [1] &quot;Name and Lastname; ID; email adress&quot; ## [2] &quot;Samuel Jackson; 10124585611; samuelJ@mail.com&quot; ## [3] &quot;Robert Downey; 4525111559; rdownironman@mail.c&quot; ## [4] &quot;Douglas Adams; 14884674721; zaphodbeeblebrox@mail.com&quot; ## [5] &quot;Stephen Wolfram; 74682868914; wolframalpha@mail.com&quot; r measure run time and optimization "],["simulation-and-modeling.html", "Chapter 7 Simulation and modeling 7.1 Simulation 7.2 sampling 7.3 Code Optimization 7.4 SWIRL practices 7.5 Final assignment", " Chapter 7 Simulation and modeling profiler is R a tool that allows you to check the performance of your functions and would help you optimize The str() function gives a quick examination of R objects functions and data properties, useful for having a general idea of what an specific expression is. str(str) library(datasets) head(airquality) str(airquality) s &lt;- split(airquality,airquality$Month) str(s) 7.1 Simulation because R is statistics orientated it has many functions used for creating distributions. Each type of distribution like normal, Poisson, etc. has associated the following prefixes that specify different outcomes. d: density __r:__random number generation p: cumulative q: quantile The following are some examples. rnorm: normal distribution given a mean and a standard deviation dnorm: Probability density pnorm: Cumulative distribution from a normal distribution *rpois: generates a random Possion distribution given a defined rate library(datasets) with(airquality,tapply(Temp,Month,mean)) ## 5 6 7 8 9 ## 65.54839 79.10000 83.90323 83.96774 76.90000 Temperature&lt;- airquality$Temp model &lt;-rnorm(153,mean(Temperature),sd(Temperature)) plot(density(Temperature)) Figure 7.1: air density plot(density(model)) Figure 7.2: air density R produces random number that are not entirely random, instead they are made through a pseudo random generator, an algorithm that will map the output to a list of values with a determined probabilistic distribution. A seed will determine the behavior of the algorithm, any seed will always map to the same numbers. It is important to be able to reproduce the exact same results, so the set.seed() function is used to ensure that the random values generated stay the same. print(&quot; set seed to 3&quot;) ## [1] &quot; set seed to 3&quot; set.seed(3) rnorm(5) ## [1] -0.9619334 -0.2925257 0.2587882 -1.1521319 0.1957828 print(&quot;seed not set &quot;) ## [1] &quot;seed not set &quot; rnorm(5) ## [1] 0.03012394 0.08541773 1.11661021 -1.21885742 1.26736872 print(&quot; set seed to 3 again&quot;) ## [1] &quot; set seed to 3 again&quot; set.seed(3) rnorm(5) ## [1] -0.9619334 -0.2925257 0.2587882 -1.1521319 0.1957828 We want to create a linear model of the form \\[\\begin{equation*} y= \\beta _1 + \\beta_2 x +\\epsilon \\tag{7.1} \\end{equation*}\\] set.seed(20) x&lt;- rnorm(100) set.seed(10) e &lt;- rnorm(100,0,2) y&lt;- 0.5 + 2*x + e plot(x,y) set.seed(10) x&lt;- rbinom(100,1,0.5) set.seed(10) e &lt;- rnorm(100,0,2) y&lt;- 0.5 + 2*x + e plot(x,y) 7.2 sampling 7.3 Code Optimization R profiler examines how much does it take to run a piece of code #Rprof(&quot;../data/Profile.out&quot;) #source(&quot;../code/covid_19.R&quot;,echo=FALSE) #Rprof(NULL) details&lt;-summaryRprof(&quot;../data/Profile.out&quot;) ## Warning in readLines(con, n = chunksize): incomplete final line found on &#39;../ ## data/Profile.out&#39; head(details$by.self) ## self.time self.pct total.time total.pct ## &quot;charToDate&quot; 3.80 73.36 3.80 73.36 ## &quot;scan&quot; 0.96 18.53 0.96 18.53 ## &quot;as.Date&quot; 0.08 1.54 3.86 74.52 ## &quot;FUN&quot; 0.04 0.77 0.08 1.54 ## &quot;mget&quot; 0.04 0.77 0.04 0.77 ## &quot;.rs.getHelpFromObject&quot; 0.02 0.39 0.24 4.63 The time spend be of tow different categories $by.self or by.total the latter has the disadvantages that it records the time full time the top level function tend to spend must of the time can What by the example we can see that must of the time is spent on converting data into Date type. we library(profvis) 7.4 SWIRL practices As part of this weeks activities the following swirl practices 7.4.1 Looking at Data when first introduce to a dataset you need to gain a broad understanding of the data, R has many functions usefull for exploratory analysis, pocking around the data and knowing what it is in there. dim() names() object.size() allows you to know how much space a dataset is occupying in memory summary() is very useful to get a sense of the data it give properties such as the mean, the median, the number of NA values, etc. table() str() 7.4.2 Simulation simulate the probability of a flip of a coin with an skewed probability of 70% head and 30% tails. We could use sample with replacement and specify to find the number of times it would land heads making 100 flip, how ever the binomial distribution function rbinom to simulate the expected value, and not only once but the times necessary set.seed(1) rbinom(1,size=100,prob=0.7) ## [1] 68 set.seed(13) heads&lt;- rbinom(500,size=100,prob=0.7) hist(heads) Figure 7.3: heads expected of 100 coin flips range(heads) ## [1] 51 84 Generate 100 groups of random numbers each one of 5 vaules generate with a Poisson distribution with mean 10 set.seed(19) rpois(5,lambda = 10) ## [1] 6 5 8 13 9 my_pois&lt;- replicate(100,rpois(5,10)) Other distributions: exponential rexp(), chi-squared rchisq(), gamma rgamma() 7.4.3 Base Graphics lattice, ggplot2 and ggvis http://varianceexplained.org/r/teach_ggplot2_to_beginners/ http://www.ling.upenn.edu/~joseff/rstudy/week4.html load data plot is to scatterplot formula interface plot(dist ~ speed, cars) xlab = xlabel, ylab=ylabel, main =title sub=My Plot Subtitle col=2 color red ?points pch=2 ?boxplot data(mtcars) boxplot(mpg~ cyl, data=mtcars) 7.5 Final assignment "],["module3.html", "Summary 7.6 Learning objectives:", " Summary Getting and Cleaning data was the third course and on it, one learns about how to gather data and pre process in order to make it useful for analysis. It is no secret that our modern world is build on data, therefore one needs to be able to access this data independently of what format it comes on. People write and store data in spreadsheets, data base or through other formats like JSON files. 7.6 Learning objectives: Find and extract data in a practical application Interface with a database through SQL syntaxes Connect with an API (may be twitter and GitHub) Connect with an API office 365 stablish a protocol for cleaning data Parsers and extract information from raw text "],["data-recipes.html", "Chapter 8 Data recipes 8.1 codebook 8.2 Downloading files 8.3 XLM 8.4 JSON 8.5 data table", " Chapter 8 Data recipes Raw vs processed data when transforming raw data into processed data it is important to remember that all the process must be recorded (cookbook) tidying: structuring datasets to facilitate analysis Tidy data is a standard way of mapping the meaning of a dataset to its structure. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes. Each variable forms column Each observation forms a row Each type of observation unit forms a table One way of organizing variables is by their role in the analysis: are values fixed by the design of the data collection, or are they measured during the course of the experiment?  Fixed variables should come first, followed by measured variables, each ordered so that related variables are contiguous problems to be faced: * Column headers are values * Multiple variables are stored in one column * Variables are stored in both rows and columns * Multiple type of observational units are stored in the same table * A single observational unit is stored in multiple tables 8.1 codebook Must contain: * A Study design section, a description on the methods used to collect the data. * Description of each and every variables to be used including its units. instruction list An script with no parameters that has the raw data as an intake and produces the processed/tidy data. if it is not possible to make all the process be done through the script the should be instruction on any additional steps. 8.2 Downloading files lets face the internet is gateway to the knowledge of the world and you might obtain must of your dataset downloading them through the internet download.file(&quot;https://google.com&quot;,&quot;google.html&quot;) dateDownloaded&lt;- date() if(!file.exists(&quot;../data&quot;)){ dir.create(&quot;../data&quot;) } work with files and directories curl ulr excel_url&lt;- &quot;https://www.ins.gov.co/BoletinesCasosCOVID19Colombia/2020-03-14.xlsx?accessType=DOWNLOAD&quot; download.file(excel_url, &quot;../data/daily-cases-covid-2020-03-14-Colombia.xlsx&quot;,method=&quot;curl&quot;) dateDownloaded&lt;- date() library(xlsx) file_path&lt;-&quot;../data/daily-cases-covid-2020-03-14-Colombia.xlsx&quot; covid&lt;- read.xlsx(file_path,sheetIndex = 1,encoding = &quot;UTF-8&quot;) student_info&lt;- read.csv(&quot;../data/student_info.csv&quot;,sep=&quot;;&quot;) write.xlsx(student_info,&quot;../data/student_info.xlsx&quot;) 8.3 XLM extensible markup language. used to store structured data. Extensibility used in web scrapping. It is composed by two parts: * The markup: the label that composes the structure * The content: the actual value store Same as HTML structure it works with tags usually there is an starting tag and an ending one, tags can hold attributes library(XML) fielURL&lt;-&quot;https://www.w3schools.com/xml/note.xml&quot; 8.4 JSON JavaScript Object Notation is way of storing data in a structured manner, used extensibility in APIs. transform data sets and turn it into JSON format library(jsonlite) repos&lt;-fromJSON(&quot;https://api.github.com/users/jsduenass/repos&quot;) gists&lt;-fromJSON(&quot;https://api.github.com/users/jsduenass/gists&quot;) names(gists) ## [1] &quot;url&quot; &quot;forks_url&quot; &quot;commits_url&quot; &quot;id&quot; &quot;node_id&quot; ## [6] &quot;git_pull_url&quot; &quot;git_push_url&quot; &quot;html_url&quot; &quot;files&quot; &quot;public&quot; ## [11] &quot;created_at&quot; &quot;updated_at&quot; &quot;description&quot; &quot;comments&quot; &quot;user&quot; ## [16] &quot;comments_url&quot; &quot;owner&quot; &quot;truncated&quot; owner&lt;-gists$owner myjson&lt;-toJSON(head(iris,3),pretty = TRUE) cat(myjson) ## [ ## { ## &quot;Sepal.Length&quot;: 5.1, ## &quot;Sepal.Width&quot;: 3.5, ## &quot;Petal.Length&quot;: 1.4, ## &quot;Petal.Width&quot;: 0.2, ## &quot;Species&quot;: &quot;setosa&quot; ## }, ## { ## &quot;Sepal.Length&quot;: 4.9, ## &quot;Sepal.Width&quot;: 3, ## &quot;Petal.Length&quot;: 1.4, ## &quot;Petal.Width&quot;: 0.2, ## &quot;Species&quot;: &quot;setosa&quot; ## }, ## { ## &quot;Sepal.Length&quot;: 4.7, ## &quot;Sepal.Width&quot;: 3.2, ## &quot;Petal.Length&quot;: 1.3, ## &quot;Petal.Width&quot;: 0.2, ## &quot;Species&quot;: &quot;setosa&quot; ## } ## ] 8.5 data table Is an analogue to data frame structure however it tends to be more optimized. The tables command (dont confuse it with table) display the currently used data.tables library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year #?getDTthreads DT&lt;-data.table(iris) class(DT) ## [1] &quot;data.table&quot; &quot;data.frame&quot; tables() ## NAME NROW NCOL MB COLS ## 1: DT 150 5 0 Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species ## KEY ## 1: ## Total: 0MB DT[,Petal.Area:=Petal.Length*Petal.Width] # adding new row head(DT,3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Petal.Area ## 1: 5.1 3.5 1.4 0.2 setosa 0.28 ## 2: 4.9 3.0 1.4 0.2 setosa 0.28 ## 3: 4.7 3.2 1.3 0.2 setosa 0.26 Expressions add new columns, if a data table is assign to a new variable they become intertwined, so any change in one will affect the other, it is better to create copy through the copy function. ## SWIRL practice ```r install_from_swirl(&quot;Getting and Cleaning Data&quot;) print(&quot;i should not show up&quot;) "],["data-formats.html", "Chapter 9 Data Formats 9.1 MySQL database 9.2 read data from HDF5 9.3 web scrapping 9.4 APIs 9.5 Catch all 9.6 QUIZ", " Chapter 9 Data Formats 9.1 MySQL database Access database information is stored in the form of tables with relations. As a data scientist youll be expected to know how to collect data from a database. Install MySQL NOTE: remember to close the connection to the database library(sqldf) ## Loading required package: gsubfn ## Loading required package: proto ## Loading required package: RSQLite 9.1.1 SQL commands SELECT: Wildcards: can be pass to extract everything LIMIT: control the number of results, analogous to `head in a data frame. ORDER BY: can order variables in ascending or descending order. _WHERE:__ limit to a condition 9.2 read data from HDF5 Hierarchical Dataset Format (HDF) used for large data sets check for recent info. write data from the data sets package know more at 9.3 web scrapping Know a little more about the http protocol and GET POST PUT commands can be access by the httr how Netflix reverse engineer Hollywood connection &lt;- url(&quot;https://en.wikipedia.org/wiki/Web_scraping&quot;) raw_htm&lt;-readLines(connection) ## Warning in readLines(connection): incomplete final line found on &#39;https:// ## en.wikipedia.org/wiki/Web_scraping&#39; close(connection) library(httr) library(XML) url&lt;- &quot;https://en.wikipedia.org/wiki/Web_scraping&quot; html&lt;- GET(url) class(html) ## [1] &quot;response&quot; scrap &lt;- content(html,as=&quot;text&quot;) class(scrap) ## [1] &quot;character&quot; parsed&lt;- htmlParse(scrap, asText = TRUE) # xpathApply(parsed) names(html) ## [1] &quot;url&quot; &quot;status_code&quot; &quot;headers&quot; &quot;all_headers&quot; &quot;cookies&quot; ## [6] &quot;content&quot; &quot;date&quot; &quot;times&quot; &quot;request&quot; &quot;handle&quot; html$cookies ## domain flag path secure expiration ## 1 #HttpOnly_en.wikipedia.org FALSE / TRUE 2021-05-15 19:00:00 ## 2 #HttpOnly_.wikipedia.org TRUE / TRUE 2021-05-15 19:00:00 ## 3 .wikipedia.org TRUE / TRUE &lt;NA&gt; ## name value ## 1 WMF-Last-Access 14-Apr-2021 ## 2 WMF-Last-Access-Global 14-Apr-2021 ## 3 GeoIP CO:DC:Bogot__:4.64:-74.07:v4 google &lt;- handle(&quot;https://www.google.com/&quot;) pg1&lt;- GET(handle=google, path=&quot; search&quot;) 9.4 APIs twitter (https://developer.twitter.com/apps) authentication keys tokens when working in open source and public code projects 9.5 Catch all When dealing with data there are many potential sources which makes unpractical to make an extensive list that cover all of them. Th key takeaway is that there probably exists a R package that would help you extract data for any source you can imagine. ?connections read images and GIS (Geographic Information System) and read music files with tuneR and seewave. 9.6 QUIZ Register an application with the Github API here https://github.com/settings/applications. Access the API to get information on your instructors repositories (hint: this is the url you want https://api.github.com/users/jtleek/repos). Use this data to find the time that the data sharing repo was created. What time was it created? This tutorial may be useful (https://github.com/hadley/httr/blob/master/demo/oauth2-github.r). You may also need to run the code in the base R package and not R studio. library(jsonlite) repos&lt;-fromJSON(&quot;https://api.github.com/users/jtleek/repos&quot;) repos$name ## [1] &quot;2018&quot; &quot;ads2020&quot; ## [3] &quot;advdatasci&quot; &quot;advdatasci-project&quot; ## [5] &quot;advdatasci-swirl&quot; &quot;advdatasci15&quot; ## [7] &quot;advdatasci16&quot; &quot;advdatasci_swirl&quot; ## [9] &quot;ballgown&quot; &quot;big_course&quot; ## [11] &quot;bookdown-start&quot; &quot;books&quot; ## [13] &quot;capitalIn21stCenturyinR&quot; &quot;careerplanning&quot; ## [15] &quot;coc&quot; &quot;courses&quot; ## [17] &quot;COVID-19&quot; &quot;crsra&quot; ## [19] &quot;cshlcg-labs&quot; &quot;data&quot; ## [21] &quot;dataanalysis&quot; &quot;datascientist&quot; ## [23] &quot;datasharing&quot; &quot;datawomenontwitter&quot; ## [25] &quot;day1&quot; &quot;derfinder&quot; ## [27] &quot;derfinder-1&quot; &quot;DSM&quot; ## [29] &quot;EDA-Project&quot; &quot;escalatr&quot; id&lt;-grep( &quot;datasharing&quot;,repos$name) repos$created_at[id] ## [1] &quot;2013-11-07T13:25:07Z&quot; The sqldf package allows for execution of SQL commands on R data frames. We will use the sqldf package to practice the queries we might send with the dbSendQuery command in RMySQL. Download the American Community Survey data and load it into an R object called https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv library(sqldf) url&lt;-&quot;https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv&quot; fpath&lt;- &quot;../data/M3_week2_quiz_data/AmericanCommunitySurvey.csv &quot; if(is.null(fpath)){ download.file(url,fpath) } acs&lt;-read.table(fpath) Which of the following commands will select only the data for the probability weights pwgtp1 with ages less than 50? "],["on-reproducible-research.html", "Chapter 10 On reproducible research", " Chapter 10 On reproducible research reproducible reporting is about is a validation of the data analysis best practices should be done to promote and encourage reproducibility, particularly in whats called omics based research, such as genomics, pro-teomics, other similar areas involving high-throughput biological measurements Peng (2016) exploratory figures you need to understand were the data came from divide the data into tow categories train data and validation data this ca be done with the binomial distribution link mydata&lt;- read.table(&quot;../data/student_info.csv&quot;,sep=&quot;;&quot;,header=TRUE) n&lt;- dim(mydata)[1] set.seed(3435) trainIndicator&lt;-rbinom(n, size=1, prob=0.5) train_data&lt;- mydata[trainIndicator,] val_data&lt;-mydata[!trainIndicator,] val_data ## Name.and.Lastname ID email.adress ## 2 Robert Downey 4525111559 rdownironman@mail.c ## 3 Douglas Adams 14884674721 zaphodbeeblebrox@mail.com ## 4 Stephen Wolfram 74682868914 wolframalpha@mail.com ## 5 Cleve Moler 17463589721 chiefmathematician@mail.com ## 6 Matt Parker 18457956247 parkersquare@mail.com ## 8 Emily Graslie 17973595287 brainscoop@mial.com ## 10 Destin Sandlin 17895782879 smarter@mail.com ## 11 Freddy Vega 17795795697 fredier@mail.com ## 13 Isaac Asimov 13589844557 robots@mail.com ## 15 R Daneel Olivaw 10001110101 dolivaw@mail.com References "],["final-regards.html", "Chapter 11 Final regards 11.1 References", " Chapter 11 Final regards 11.1 References "]]
